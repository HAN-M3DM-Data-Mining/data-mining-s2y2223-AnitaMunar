---
title: "ind_exam"
format: html
editor: visual
---

## INDIVIDUAL ASSIGNMENT

Anita MunÃ¡r (1652126)

In this following document, I present my code for the individual data analytics assignment. The goal of this project is to find out what makes for a high "work-life balance".

## Business Understanding

The study which served as a basis for this study is called Lifestyle_and_Wellbeing data, uploaded to Kaggle on March 14th, 2021. The survey is still active, and can be reached at https://www.authentic-happiness.com/your-authentic-happiness-score.

The dataframe is consists of the 15,977 recorded survey responses on 24 attributes describing how indiviuals thrive in their professional and personal lives. It reflects how well respondents are prrforming within five subgroups: Body, Mind, Expertise, Connection and Meaning. The goal of the researchers was to find out how can an individual optimize or even reinvent their lifestyle in order to achieve a higher sense of a balanced life.

-   In recent years, the topic of work-life balance have raised the interest of many HR professionals,due to... (DO RESEARCH)

Below, I load in the dataset and the libraries that I need in order to run a meaningful data analytics project in R studio.

```{r}
library(class)
library(ggplot2)
library(caret)
library(lattice)
library(tidyverse)
library(RColorBrewer)
library(tm)
library(slam)
library(e1071)
library(ISLR)
library(magrittr)
library(dplyr)
library(car)
library(stats)
library(reshape2)
library(randomForest)
```

Brief description of these libraries:

1.  class: The 'class' library provides various functions for classification tasks, such as k-nearest neighbors (k-NN) classification.

2.  ggplot2: 'ggplot2' is a powerful visualization library that allows for creating highly customizable and publication-quality plots and graphics.

3.  caret: The 'caret' library is used for machine learning and provides a unified interface for training and evaluating various models. It also includes functions for data preprocessing, feature selection, and model tuning.

4.  lattice: 'lattice' is a plotting library that provides a high-level interface for creating Trellis plots, which are useful for visualizing multivariate data.

5.  tidyverse: 'tidyverse' is a collection of packages, including 'ggplot2' and 'dplyr', that work together to provide a consistent and efficient approach to data manipulation, exploration, and visualization.

6.  RColorBrewer: 'RColorBrewer' offers a set of color palettes for creating visually appealing and distinct color schemes in plots.

7.  tm: The 'tm' library is used for text mining and provides tools for preprocessing, analyzing, and visualizing text data.

8.  slam: 'slam' provides functions for sparse and dense matrix computations, particularly useful for handling large and sparse matrices efficiently.

9.  e1071: The 'e1071' library offers implementations of various machine learning algorithms, including support vector machines (SVMs) and naive Bayes classifiers.

10. ISLR: The 'ISLR' library contains datasets used in the book "An Introduction to Statistical Learning with Applications in R." It is a valuable resource for learning and practicing statistical modeling and machine learning techniques.

11. magrittr: 'magrittr' provides a set of pipe operators (%\>%, %\>%), enabling a more readable and intuitive coding style by chaining multiple operations together.

12. dplyr: 'dplyr' is a widely-used library for data manipulation tasks, offering a grammar of data manipulation functions that facilitate filtering, transforming, summarizing, and joining datasets.

13. car: The 'car' library offers various functions for applied regression modeling, including diagnostic tools, hypothesis testing, and regression model visualizations.

14. stats: 'stats' is a core R library that provides a wide range of statistical functions and distributions, including basic descriptive statistics, hypothesis testing, and probability distributions.

15. reshape2: the correlation matrix is calculated and then reshaped using **`melt()`** from the **`reshape2`** package

16. randomForest: the randomForest package in R is a popular and powerful tool for building random forest models, which are a type of ensemble learning method for regression and classification tasks. The randomForest package provides functions to create, tune, and evaluate random forest models.

Overall, these libraries offer a comprehensive selection of tools and functions to handle data preprocessing, exploration, visualization, statistical modeling, and machine learning tasks in R. As a data scientist, it's essential to be familiar with these libraries in order to effectively analyze and interpret data.

```{r}
# Read the dataset from the CSV file
data <- read.csv("https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/data-mining-s2y2223-AnitaMunar/master/Wellbeing_and_lifestyle_data_Kaggle.csv")
```

## Data Understanding

In this section, I take a look at the dataset to see its initial state, and identify the meaning of each variable. The original dataset contains **15972** observations on **24** variables.

```{r}
# Explore the structure and summary statistics of the dataset
str(data)
```

```{r}
summary(data)
```

**WORK_LIFE_BALANCE**: Numeric data, representing the given score of each respondent. Scale from 480 to 820. This score is generated by the algorithm of Authentic Happiness (https://www.authentic-happiness.com/). This will be the dependent variable.

**Timestamp**: Date logs, stored in character type format as seen: "7/7/15" representing day/month/year.

**FRUITS_VEGGIES**: Integer data, respondents' choice fro 0-5 as for the daily servings of fruits or vegetables consumed.

**DAILY_STRESS**: Character data, due to a mistake which will be corrected in later phase. Respondents' choice from a scale of 0-5 with 0 being "Not much stress" and 5 being "A lot of stress".

**PLACES_VISITED**: Integer data, representing the number of new places the respondent visited in the past 12 months' time. Scale of 0-10, with 0 being "none" and 10 representing 10 or more new places.

**CORE_CIRCLE**: Integer data, on a scale of 0 to 10 (or more) representing the number of people the respondents feel close to, such as family and friends who provide long-term support.

**SUPPORTING_OTHERS**: Integer data, reflecting the number of people the respondent has helped with his or her alturism in the last 12 months. This means supporting a friend caring about a family member, etc. Scale of 0 to 10, with 10 representing 10 or more people helped.

**SOCIAL_NETWORK**: Integer data, scale of 0 to 10 representing the number of people with whom respondents have interaction on a daily basis.

**ACHIEVEMENT**: Integer data on a scale of 0 to 10, representing the number of achievements gained by the respondents in the past 12 months and are known by either friends, family or coworkers.

**DONATION:** Integer data, scale of 0 to 10 representing the number of donations made in the past 12 months by respondents. financial donation, time contribution, fundraising, volunteering and other forms of serving one's country or serving those in need.

**BMI_RANGE**: Integer data repreenting two options: below 25 and over 25. These are coded as "1" and "2" respectively, and indicate whether the respondent is close to being overweight (2) or not (1), calculated from their weight, height and gender.

**TODO_COMPLETED**: Integer data, representing how well the respondent conducts their weekly to-do list. Scale of 0 to 10, with 0 being "not at all", and 10 being "very well".

**FLOW**: Integer data on a scale of 0-10, representing hours of flow experience daily. "'Flow' is defined as the mental state, in which you are fully immersed in performing an activity."

**DAILY_STEPS**: Integer data, scale of 1-10. This is represented by thousands of steps, with 1 being "Less than 1,000 steps" and 10 being "Thousand steps" (or more) taken by the respondent daily.

**LIVE_VISION**: Integer data, scale of 0-10 with 0 being "I do not have a life vision" and 10 being "10 Years or more", as in how many years ahead the respondent is actively planning or has a vision for.

**SLEEP_HOURS**: Integer data, scale of 1-10 representing the number of hours slept at night on a weekly average.

**LOST_VACATION**: Integer data, scale of 0-10 representing the yearly vacation days lost due to illness, anxiety or other personal circumstances.

**DAILY_SHOUTING**: Integer data, despite being coded as "daily", this variable represents the weekly occasion of shouting or sulking experienced by the respondent. Scale of 0-10, representing number of occasions.

**SUFFICIENT_INCOME**: Integer data representing two options: "not or hardly sufficient" being "1" and "sufficient" being "2", reflecting on the costs of housing, groceries, education and healthcare and their affordability from the respondent's wage.

**PERSONAL_AWARDS**: Integer data, scale of 0-10 representing the number of recognition received validating a level of professionalism or level of engagement. Examples include degree, certificate, accreditation, award, prize, etc. The variable measures the number of these awards received by the respondent in the course of the last 12 months.

**TIME_FOR_PASSION**: Integer data, scale of 0-10, representing hours spent daily on things that the respondent is passionate about and possibly contributes to an elevated/elevating feeling.

**WEEKLY_MEDITATION**: Integer data, scale of 0-10. This variable represents the nmber of occasions per week where the respondent feels that they have time for themselves, such as meditation, fitness, and relaxation activities.

**AGE**: Character data, from the 4 given options: "Less than 20", "21 to 35" "36 to 50" and lastly: "51 or more".

**GENDER**: Character data, respondents can indicate their gender as "Female" and "Male".

After seeing how the data looks like, certain steps need to be taken in order to prepare it for running data analytics models afterwards.

## Data Preparation

First, I start by removing rows with NA values to avoid missing data. The main dataframe will be named "selected_data"

```{r}
# Separate raw data
selected_data <- data
# Check for missing values
sum(is.na(selected_data))
```

I confirm that there were no missing data in the original dataset.

### Variable recoding

#### Daily Stress

Next, I remember seeing the variable "DAILY_STRESS" being coded as character data instead of integer. This must have been a small error, which can be corrected with:

```{r}
# Convert "DAILY_STRESS" column to integer
selected_data$DAILY_STRESS <- as.integer(selected_data$DAILY_STRESS)

#check after conversion
str(selected_data)
```

This confirms that the conversion worked. Although, I get a warning about NAs.

```{r}
sum(is.na(selected_data))
```

I choose to delete that row, since something must've gone wrong there and it is small relatively to the whole sample.

```{r}
# Remove rows with missing values
selected_data <- na.omit(selected_data)
sum(is.na(selected_data))
```

As of now, the dataset in use contains **15971** observations on **24** variables.

#### Gender

Next, for gender:

```{r}
# Convert "Female" to 0 and "Male" to 1
selected_data$GENDER <- ifelse(selected_data$GENDER == "Female", 0, 1)
str(selected_data)

```

```{r}
sum(is.na(selected_data))
```

#### Age

Next, recoding the Age variable's four choices into integer data:

```{r}
# Recode age variable
selected_data$AGE <- ifelse(selected_data$AGE %in% c("Less than 20"), 1,
                             ifelse(selected_data$AGE %in% c("21 to 35"), 2,
                                    ifelse(selected_data$AGE %in% c("36 to 50"), 3,
                                           ifelse(selected_data$AGE %in% c("51 or more"), 4, NA))))

# Convert AGE column to integer
selected_data$AGE <- as.integer(selected_data$AGE)

str(selected_data)
```

```{r}
sum(is.na(selected_data))
```

#### Timestamp

Next, the timestamp variable is converted to a format that recognizes. After checking the dataset manually, it is visible that the format is currently month/day/year. Therefore, that is how I will code it:

```{r}
selected_data$Timestamp <- as.Date(selected_data$Timestamp, format = "%m/%d/%y")
str(selected_data)

```

Now we omit the empty values again:

```{r}
# Check for missing values
sum(is.na(selected_data))

```

```{r}
str(selected_data)
```

### Distributions

So far, the cleaning goes well. Now the means of integer variables and other kinds of distribution visualizations can be omitted.

```{r}
# Filter integer variables
integer_vars <- selected_data[, sapply(selected_data, is.integer)]

# Run summary statistics
summary(integer_vars)

```

```{r}

# Calculate the means of integer variables
means <- colMeans(integer_vars, na.rm = TRUE)

# Create a dataframe for plotting
plot_data <- data.frame(variable = names(means), mean = means)

ggplot(plot_data, aes(x = variable, y = mean)) +
  geom_bar(stat = "identity", fill = "steelblue", width = 0.5) +
  xlab("") +
  ylab("Mean") +
  ggtitle("Mean of Integer Variables") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Using the ggplot library, the means of all variables can be viewed. The results show a reasonable distribution with respect to the original scale of each variable.

### Gender distribution

```{r}
# Convert GENDER variable to factor with appropriate labels
selected_data$GENDER <- factor(selected_data$GENDER, levels = c(0, 1), labels = c("Female", "Male"))

# Create a data frame with gender counts
gender_counts <- table(selected_data$GENDER)
gender_data <- data.frame(Gender = names(gender_counts), Count = as.numeric(gender_counts))

# Calculate percentages
gender_data$Percentage <- round((gender_data$Count / sum(gender_data$Count)) * 100, 2)

# Specify fill colors and labels for genders
gender_colors <- c("#FF69B4", "#6495ED") # Pink for Female, Blue for Male
gender_labels <- c("Female", "Male")

# Pie chart for GENDER variable
pie_chart_gender <- ggplot(data = gender_data,
                           aes(x = "", y = Count, fill = Gender)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = gender_colors, labels = gender_labels) +
  ggtitle("Distribution of Gender") +
  theme_void() +
  geom_text(aes(label = paste0(Percentage, "%")),
            position = position_stack(vjust = 0.5), color = "white")

# Display the pie chart
print(pie_chart_gender)

```

As the pie chart shows, there is somewhat more women than men in among the participants.

### WLB distribution

```{r}
# Mean of WORK_LIFE_BALANCE variable
mean_work_life_balance <- mean(selected_data$WORK_LIFE_BALANCE, na.rm = TRUE)
print(paste("Mean of Work Life Balance:", round(mean_work_life_balance, 2)))
```

The mean of the work life balance is found to be 666.

```{r}
# Histogram for WORK_LIFE_BALANCE variable
histogram_work_life_balance <- ggplot(data = selected_data, aes(x = WORK_LIFE_BALANCE_SCORE)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(x = "Work Life Balance", y = "Frequency") +
  ggtitle("Distribution of Work Life Balance") +
  theme_minimal()

# Display the histogram
print(histogram_work_life_balance)

```

There seems to be a normal distribution for the work life balance scores, indicated by the bell shape of the histogram.

### Individual distributions

Lets look at the individual distribution of variables:

```{r}
# Create a histogram for FRUITS_VEGGIES
ggplot(selected_data, aes(FRUITS_VEGGIES)) +
  geom_histogram(fill = "coral", color = "black", bins = 6) +
  labs(title = "Distribution of FRUITS_VEGGIES") +
  theme_minimal()
```

For the fruits and veggies variable, there is almost a normal distribution, but there seems to be a lot of respondents who eat 5 or more fresh produce daily.

```{r}
# Create a bar plot for DAILY_STRESS
ggplot(selected_data, aes(x = as.factor(DAILY_STRESS))) +
  geom_bar(fill = "maroon", color = "black") +
  labs(title = "Distribution of DAILY_STRESS") +
  theme_minimal()
```

The daily stress variable shows a normal distribution, slightly left-skewed. This indicates that respondents are leaning to a higher score on observed stress levels.

```{r}
# Create a histogram for PLACES_VISITED
ggplot(selected_data, aes(x = PLACES_VISITED)) +
  geom_histogram(fill = "steelblue", color = "black", bins = 11) +
  labs(title = "Distribution of PLACES_VISITED") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

There seems to be a lot of respondents who visited 10 or more places in the last year. Hopefully, this is not an error, but it is worth looking into.

```{r}
# Set the range of threshold values to test
threshold_values <- seq(3, 10, by = 0.5)

# Perform sensitivity analysis
results <- data.frame(Threshold = threshold_values, OutlierCount = NA)

for (i in seq_along(threshold_values)) {
  # Detect outliers based on the current threshold value for PLACES_VISITED
  outliers <- selected_data[selected_data$PLACES_VISITED >= threshold_values[i], ]
  
  # Record the count of outliers
  results$OutlierCount[i] <- nrow(outliers)
}

# Print the sensitivity analysis results
print(results)

```

```{r}
# Set the range of threshold values to test
threshold_values <- seq(3, 10, by = 0.5)

# Detect outliers based on the threshold values for PLACES_VISITED
outlier_counts <- sapply(threshold_values, function(threshold) {
  sum(selected_data$PLACES_VISITED >= threshold)
})

# Create the results data frame
results <- data.frame(Threshold = threshold_values, OutlierCount = outlier_counts)

# Print the sensitivity analysis results
print(results)

```

According to the sensitivity analysis, the scores of 10 are not due to an error.

Now, lets look at the distributions of each score further:

```{r}
# Create a histogram for CORE_CIRCLE
ggplot(selected_data, aes(x = CORE_CIRCLE)) +
  geom_histogram(fill = "navy", color = "black", bins = 11) +
  labs(title = "Distribution of CORE_CIRCLE") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for SUPPORTING_OTHERS
ggplot(selected_data, aes(x = SUPPORTING_OTHERS)) +
  geom_histogram(fill = "gold", color = "black", bins = 11) +
  labs(title = "Distribution of SUPPORTING_OTHERS") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for SOCIAL_NETWORK
ggplot(selected_data, aes(x = SOCIAL_NETWORK)) +
  geom_histogram(fill = "cyan", color = "black", bins = 11) +
  labs(title = "Distribution of SOCIAL_NETWORK") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for ACHIEVEMENT
ggplot(selected_data, aes(x = ACHIEVEMENT)) +
  geom_histogram(fill = "red", color = "black", bins = 11) +
  labs(title = "Distribution of ACHIEVEMENT") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for DONATION
ggplot(selected_data, aes(x = DONATION)) +
  geom_histogram(fill = "magenta", color = "black", bins = 11) +
  labs(title = "Distribution of DONATION") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for BMI_RANGE
ggplot(selected_data, aes(x = BMI_RANGE)) +
  geom_bar(fill = "purple", color = "black") +
  labs(title = "Distribution of BMI_RANGE") +
  scale_x_continuous(breaks = c(1, 2), labels = c("Below 25", "Over 25")) +
  theme_minimal()

```

```{r}
# Create a histogram for TODO_COMPLETED
ggplot(selected_data, aes(x = TODO_COMPLETED)) +
  geom_histogram(fill = "peru", color = "black", bins = 11) +
  labs(title = "Distribution of TODO_COMPLETED") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for FLOW
ggplot(selected_data, aes(x = FLOW)) +
  geom_histogram(fill = "grey", color = "black", bins = 11) +
  labs(title = "Distribution of FLOW") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a bar plot for DAILY_STEPS
ggplot(selected_data, aes(x = DAILY_STEPS)) +
  geom_bar(fill = "darksalmon", color = "black") +
  labs(title = "Distribution of DAILY_STEPS") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()

```

```{r}
# Create a histogram for LIVE_VISION
ggplot(selected_data, aes(x = LIVE_VISION)) +
  geom_histogram(fill = "chartreuse", color = "black", bins = 11) +
  labs(title = "Distribution of LIVE_VISION") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a bar plot for SLEEP_HOURS
ggplot(selected_data, aes(x = SLEEP_HOURS)) +
  geom_bar(fill = "darkgreen", color = "black") +
  labs(title = "Distribution of SLEEP_HOURS") +
  scale_x_continuous(breaks = seq(1, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for LOST_VACATION
ggplot(selected_data, aes(x = LOST_VACATION)) +
  geom_histogram(fill = "wheat", color = "black", bins = 11) +
  labs(title = "Distribution of LOST_VACATION") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for DAILY_SHOUTING
ggplot(selected_data, aes(x = DAILY_SHOUTING)) +
  geom_histogram(fill = "darkorange", color = "black", bins = 11) +
  labs(title = "Distribution of DAILY_SHOUTING") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a bar plot for SUFFICIENT_INCOME
ggplot(selected_data, aes(x = as.factor(SUFFICIENT_INCOME))) +
  geom_bar(fill = "lightcyan", color = "black") +
  labs(title = "Distribution of SUFFICIENT_INCOME") +
  scale_x_discrete(labels = c("Insufficient", "Sufficient")) +
  theme_minimal()

```

```{r}
# Create a histogram for PERSONAL_AWARDS
ggplot(selected_data, aes(x = PERSONAL_AWARDS)) +
  geom_histogram(fill = "lightpink", color = "black", bins = 11
                 ) +
  labs(title = "Distribution of PERSONAL_AWARDS") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()
```

```{r}
# Create a histogram for TIME_FOR_PASSION
ggplot(selected_data, aes(x = TIME_FOR_PASSION)) +
  geom_histogram(fill = "yellow", color = "black", bins = 11) +
  labs(title = "Distribution of TIME_FOR_PASSION") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()

```

```{r}
# Create a histogram for WEEKLY_MEDITATION
ggplot(selected_data, aes(x = WEEKLY_MEDITATION)) +
  geom_histogram(fill = "linen", color = "black", bins = 11) +
  labs(title = "Distribution of WEEKLY_MEDITATION") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  theme_minimal()

```

```{r}
# Create a histogram for AGE with relabeled x-axis labels
ggplot(selected_data, aes(x = factor(AGE))) +
  geom_bar(fill = "khaki", color = "black") +
  labs(title = "Distribution of AGE") +
  scale_x_discrete(labels = c("Less than 20", "21 to 35", "36 to 50", "51 or more")) +
  theme_minimal()

```

#### Score frequency test for 10s

Is it natural that some variables have very frequent score 10s?

```{r}
# Specify the specific score to check for
specific_score <- 10

# Iterate over each column in the selected_data dataset
for (col in names(selected_data)) {
  # Check if the specific score exists in the column
  if (!(specific_score %in% selected_data[[col]])) {
    cat("Column:", col, "does not contain the specific score\n")
    next
  }
  
  # Calculate the proportion of the specific score in the column
  score_proportion <- sum(selected_data[[col]] == specific_score) / length(selected_data[[col]])
  
  # Print the column name and the proportion for all columns
  cat("Column:", col, "\tProportion:", score_proportion, "\n")
}

```

After conversion, the percentages of score 10s in each variable (that contained 10s) in descending order are the following

1.  SOCIAL_NETWORK: 34.16%

2.  WEEKLY_MEDITATION: 26.83%

3.  SUPPORTING_OTHERS: 25.00%

4.  PERSONAL_AWARDS: 23.57%

5.  PLACES_VISITED: 22.28%

6.  CORE_CIRCLE: 17.43%

7.  DAILY_STEPS: 16.91%

8.  LOST_VACATION: 14.01%

9.  LIVE_VISION: 13.57%

10. ACHIEVEMENT: 7.33%

11. TODO_COMPLETED: 6.78%

12. DAILY_SHOUTING: 4.92%

13. TIME_FOR_PASSION: 4.27%

14. FLOW: 2.12%

Therefore, the following variables have resulted in more than a quarter of all responses being 10.

1.  SOCIAL_NETWORK: 34.16%

2.  WEEKLY_MEDITATION: 26.83%

3.  SUPPORTING_OTHERS: 25.00%

This gives some cause for concern. The dataset is huge and has been collected over long years over the internet. The chances of people naturally scoring this optimistically on these 3 segments are low.

### Yearly changes

Scores of variables per date, using dpylr package:

```{r}
# Extract the year from the timestamp variable
selected_data$Year <- format(selected_data$Timestamp, "%Y")

# Create a new dataframe with the averages by year
averages_by_year <- selected_data %>%
  group_by(Year) %>%
  summarize(across(everything(), mean, na.rm = TRUE))

# Print the new dataframe with averages by year
print(averages_by_year)

```

#### Graphs

Visualizing the yearly spread per averages for each variable:

```{r}
# FRUITS_VEGGIES
ggplot(averages_by_year, aes(x = Year, y = FRUITS_VEGGIES, group = 1)) +
  geom_point(color = "coral") +
  geom_line(color = "coral") +
  labs(title = "Yearly Changes in FRUITS_VEGGIES") +
  theme_minimal()
```

```{r}
# DAILY_STRESS
ggplot(averages_by_year, aes(x = Year, y = DAILY_STRESS, group = 1)) +
  geom_point(color = "maroon") +
  geom_line(color = "maroon") +
  labs(title = "Yearly Changes in DAILY_STRESS") +
  theme_minimal()
```

```{r}
# PLACES_VISITED
ggplot(averages_by_year, aes(x = Year, y = PLACES_VISITED, group = 1)) +
  geom_point(color = "steelblue") +
  geom_line(color = "steelblue") +
  labs(title = "Yearly Changes in PLACES_VISITED") +
  theme_minimal()
```

```{r}
# CORE_CIRCLE
ggplot(averages_by_year, aes(x = Year, y = CORE_CIRCLE, group = 1)) +
  geom_point(color = "navy") +
  geom_line(color = "navy") +
  labs(title = "Yearly Changes in CORE_CIRCLE") +
  theme_minimal()
```

```{r}
# SUPPORTING_OTHERS
ggplot(averages_by_year, aes(x = Year, y = SUPPORTING_OTHERS, group = 1)) +
  geom_point(color = "black") +
  geom_line(color = "gold") +
  labs(title = "Yearly Changes in SUPPORTING_OTHERS") +
  theme_minimal()
```

```{r}
# SOCIAL_NETWORK
ggplot(averages_by_year, aes(x = Year, y = SOCIAL_NETWORK, group = 1)) +
  geom_point(color = "cyan") +
  geom_line(color = "cyan") +
  labs(title = "Yearly Changes in SOCIAL_NETWORK") +
  theme_minimal()
```

```{r}
# ACHIEVEMENT
ggplot(averages_by_year, aes(x = Year, y = ACHIEVEMENT, group = 1)) +
  geom_point(color = "red") +
  geom_line(color = "red") +
  labs(title = "Yearly Changes in ACHIEVEMENT") +
  theme_minimal()
```

```{r}
# DONATION
ggplot(averages_by_year, aes(x = Year, y = DONATION, group = 1)) +
  geom_point(color = "magenta") +
  geom_line(color = "magenta") +
  labs(title = "Yearly Changes in DONATION") +
  theme_minimal()
```

```{r}
# BMI_RANGE
ggplot(averages_by_year, aes(x = Year, y = BMI_RANGE, group = 1)) +
  geom_point(color = "purple") +
  geom_line(color = "purple") +
  labs(title = "Yearly Changes in BMI_RANGE") +
  theme_minimal()
```

```{r}
# TODO_COMPLETED
ggplot(averages_by_year, aes(x = Year, y = TODO_COMPLETED, group = 1)) +
  geom_point(color = "peru") +
  geom_line(color = "peru") +
  labs(title = "Yearly Changes in TODO_COMPLETED") +
  theme_minimal()
```

```{r}
# FLOW
ggplot(averages_by_year, aes(x = Year, y = FLOW, group = 1)) +
  geom_point(color = "grey") +
  geom_line(color = "grey") +
  labs(title = "Yearly Changes in FLOW") +
  theme_minimal()
```

```{r}
# DAILY_STEPS
ggplot(averages_by_year, aes(x = Year, y = DAILY_STEPS, group = 1)) +
  geom_point(color = "darksalmon") +
  geom_line(color = "darksalmon") +
  labs(title = "Yearly Changes in DAILY_STEPS") +
  theme_minimal()
```

```{r}
# LIVE_VISION
ggplot(averages_by_year, aes(x = Year, y = LIVE_VISION, group = 1)) +
  geom_point(color = "chartreuse") +
  geom_line(color = "chartreuse") +
  labs(title = "Yearly Changes in LIVE_VISION") +
  theme_minimal()
```

```{r}
# SLEEP_HOURS
ggplot(averages_by_year, aes(x = Year, y = SLEEP_HOURS, group = 1)) +
  geom_point(color = "darkgreen") +
  geom_line(color = "darkgreen") +
  labs(title = "Yearly Changes in SLEEP_HOURS") +
  theme_minimal()
```

```{r}
# LOST_VACATION
ggplot(averages_by_year, aes(x = Year, y = LOST_VACATION, group = 1)) +
  geom_point(color = "black") +
  geom_line(color = "wheat") +
  labs(title = "Yearly Changes in LOST_VACATION") +
  theme_minimal()
```

```{r}
# DAILY_SHOUTING
ggplot(averages_by_year, aes(x = Year, y = DAILY_SHOUTING, group = 1)) +
  geom_point(color = "darkorange") +
  geom_line(color = "darkorange") +
  labs(title = "Yearly Changes in DAILY_SHOUTING") +
  theme_minimal()
```

```{r}
# SUFFICIENT_INCOME
ggplot(averages_by_year, aes(x = Year, y = SUFFICIENT_INCOME, group = 1)) +
  geom_point(color = "black") +
  geom_line(color = "lightcyan") +
  labs(title = "Yearly Changes in SUFFICIENT_INCOME") +
  theme_minimal()
```

```{r}
# PERSONAL_AWARDS
ggplot(averages_by_year, aes(x = Year, y = PERSONAL_AWARDS, group = 1)) +
  geom_point(color = "lightpink") +
  geom_line(color = "lightpink") +
  labs(title = "Yearly Changes in PERSONAL_AWARDS") +
  theme_minimal()
```

```{r}
# TIME_FOR_PASSION
ggplot(averages_by_year, aes(x = Year, y = TIME_FOR_PASSION, group = 1)) +
  geom_point(color = "black") +
  geom_line(color = "yellow") +
  labs(title = "Yearly Changes in TIME_FOR_PASSION") +
  theme_minimal()
```

```{r}
# WEEKLY_MEDITATION
ggplot(averages_by_year, aes(x = Year, y = WEEKLY_MEDITATION, group = 1)) +
  geom_point(color = "black") +
  geom_line(color = "linen") +
  labs(title = "Yearly Changes in WEEKLY_MEDITATION") +
  theme_minimal()
```

```{r}
# AGE
ggplot(averages_by_year, aes(x = Year, y = AGE, group = 1)) +
  geom_point(color = "black") +
  geom_line(color = "khaki") +
  labs(title = "Yearly Changes in AGE") +
  theme_minimal()
```

### WLB grouping

Divide into groups of WLB, based on "Lowest", "Low", "Mid", "High", "Highest" score:

```{r}
# Define the score groups and breakpoints
score_groups <- c("Lowest", "Low", "Mid", "High", "Highest")
breakpoints <- quantile(selected_data$WORK_LIFE_BALANCE, probs = seq(0, 1, length.out = 6), na.rm = TRUE)

# Create empty data frames for each score group
df_lowest <- data.frame()
df_low <- data.frame()
df_mid <- data.frame()
df_high <- data.frame()
df_highest <- data.frame()

# Assign observations to the corresponding score group data frames
for (group in score_groups) {
  group_data <- selected_data[selected_data$WORK_LIFE_BALANCE >= breakpoints[which(score_groups == group)] &
                              selected_data$WORK_LIFE_BALANCE <= breakpoints[which(score_groups == group) + 1], ]
  
  # Assign to the corresponding data frame
  if (group == "Lowest") {
    df_lowest <- group_data
  } else if (group == "Low") {
    df_low <- group_data
  } else if (group == "Mid") {
    df_mid <- group_data
  } else if (group == "High") {
    df_high <- group_data
  } else if (group == "Highest") {
    df_highest <- group_data
  }
}

```

Now I look at the highest WLB scoring repsondents' summary, namely the means and medians for the variables. From this, we can get a hint on what drives the score up, according to the original researchers who calculated the score.

```{r}
highest_stats <- summary(df_highest)
print(highest_stats)

```

Lets take a closer look at means and medians:

The mean shows the average of the given numeric set.

```{r}
numeric_vars <- sapply(df_highest, is.numeric)
means_highest <- colMeans(df_highest[, numeric_vars], na.rm = TRUE)
print("Means:")
print(means_highest)

```

Next, the median. The median shows the center value of the given numeric set if ordered from lowest to highest.

```{r}
numeric_vars <- sapply(df_highest, is.numeric)
medians_highest <- apply(df_highest[, numeric_vars], 2, median, na.rm = TRUE)
print("Medians:")
print(medians_highest)

```

### Correlations

Here the multicollinearity between variables is checed, using a correlation matrix witht he help of the cor() function.

```{r}
# Select only the numeric columns from selected_data
numeric_data <- selected_data[, sapply(selected_data, is.numeric)]

# Calculate the correlation matrix
cor_matrix <- cor(numeric_data)

# Print the correlation matrix
print(cor_matrix)

```

#### Heatmap

```{r}
# Select only numeric variables
numeric_vars <- sapply(selected_data, is.numeric)
numeric_data <- selected_data[, numeric_vars]

# Calculate correlation matrix
cor_matrix <- cor(numeric_data)

# Reshape correlation matrix for visualization
cor_matrix_melted <- reshape2::melt(cor_matrix)

# Create heatmap
ggplot(data = cor_matrix_melted, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))


```

In this code, after selecting the numeric variables, the correlation matrix is calculated and then reshaped using **`melt()`** from the **`reshape2`** package. The melted data is then used to create the heatmap using **`ggplot2`** functions. The visualization will show the correlation between numeric variables in the **`selected_data`** dataframe.

High and moderate correlations:

```{r}
# Set the correlation threshold
threshold <- 0.3

# Create a copy of the correlation matrix
cor_matrix_filtered <- cor_matrix

# Set correlations below the threshold to NA
cor_matrix_filtered[abs(cor_matrix_filtered) < threshold] <- NA

# Print the filtered correlation matrix
print(cor_matrix_filtered)

```

With the code above, the high and moderate correlations (\>0,3) can be omitted.

## Modelling

In this phase the models are made in order to find correlation and causation. First, the dataset must be set into training and test data, on a 80-20% basis respectively.

```{r}
# Split the dataset into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(selected_data$WORK_LIFE_BALANCE_SCORE, p = 0.8, list = FALSE)
train_data <- selected_data[train_index, ]
test_data <- selected_data[-train_index, ]
```

Next, the regression models are applied.

### Linear regression

```{r}
model_age <- lm(WORK_LIFE_BALANCE_SCORE ~ AGE, data = train_data)
summary(model_age)

```

For each increase of age group, the WLB was found to increase by 5.6 points. The p-value shows significance. Regardless, age in itself does not answer for the variablility in the data, only of around 1% of it.

```{r}
model_gender <- lm(WORK_LIFE_BALANCE_SCORE ~ GENDER, data = train_data)
summary(model_gender)

```

This suggests that being male leads to a lower WLB score by 3.6 points. The low p-value means that gender is associated with WLB, but the low R squared indicates that it is not signifcantly responsible for the variance in the data.

```{r}
model_fruits_veggies <- lm(WORK_LIFE_BALANCE_SCORE ~ FRUITS_VEGGIES, data = train_data)
summary(model_fruits_veggies)

```

The score for fruits and vegetables intake hints that one unit increase in the response for this variable led to a 14 point increase in WLB score. The p-value hints statistical relevance and the R-squared values explain that this variable is responsible for around 20.6% of the variability in WLB scores, indicating a moderate relationship.

```{r}
model_bmi_range <- lm(WORK_LIFE_BALANCE_SCORE ~ BMI_RANGE, data = train_data)
summary(model_bmi_range)

```

The coefficient for BMI_RANGE (Estimate: -23.3119) indicates that a one-unit increase in the BMI_RANGE variable (which represents being overweight - over value of 25) is associated with an estimated decrease of 23 points in the WLB score. This is quite significant which is hinted by the p-value as well but does not answer for the variance in the data scores.

```{r}
model_daily_steps <- lm(WORK_LIFE_BALANCE_SCORE ~ DAILY_STEPS, data = train_data)
summary(model_daily_steps)

```

The coefficient for DAILY_STEPS (Estimate: 6.5312) indicates that a one-unit increase in the DAILY_STEPS variable (which represents the number of steps taken per day) is associated with an estimated increase of 6.5312 in the WLB score. This variable also does not answer for the variance in the data scores, hinted by the low R-squared values.

```{r}
model_sleep_hours <- lm(WORK_LIFE_BALANCE_SCORE ~ SLEEP_HOURS, data = train_data)
summary(model_sleep_hours)

```

The coefficient for SLEEP_HOURS (Estimate: 7.5179) indicates that a one-unit increase in the SLEEP_HOURS variable (which represents the number of hours of sleep) is associated with an estimated increase of 7.5179 in the WLB score. Although it has a statistically relevant p-value, it only answers for 4% of the variance in the data, which is low.

```{r}
model_daily_stress <- lm(WORK_LIFE_BALANCE_SCORE ~ DAILY_STRESS, data = train_data)
summary(model_daily_stress)

```

A one-unit increase in DAILY_STRESS is associated with a decrease of 11.9 points in the WORK_LIFE_BALANCE_SCORE. This is rather high influence. The relationship is not likely due to chance, but this variable in itself does not answer for the variance in the data.

```{r}
model_flow <- lm(WORK_LIFE_BALANCE_SCORE ~ FLOW, data = train_data)
summary(model_flow)

```

A one-unit increase in FLOW is associated with an increase of 9.15 points in the WORK_LIFE_BALANCE_SCORE. This is a rather high influence as well in change of points. Also, the variance is significant here as well. The R-squared value indicates that approximately 22.84% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with FLOW.

```{r}
model_weekly_meditation <- lm(WORK_LIFE_BALANCE_SCORE ~ WEEKLY_MEDITATION, data = train_data)
summary(model_weekly_meditation)

```

In this case, a one-unit increase in WEEKLY_MEDITATION is associated with an increase of 6.2 points in the WORK_LIFE_BALANCE_SCORE. The R-squared value indicates that approximately 17.27% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with WEEKLY_MEDITATION.

```{r}
model_daily_shouting <- lm(WORK_LIFE_BALANCE_SCORE ~ DAILY_SHOUTING, data = train_data)
summary(model_daily_shouting)

```

In this case, a one-unit increase in DAILY_SHOUTING is associated with a decrease of 4.5 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value indicates that approximately 7.21% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with DAILY_SHOUTING.

```{r}
model_live_vision <- lm(WORK_LIFE_BALANCE_SCORE ~ LIVE_VISION, data = train_data)
summary(model_live_vision)

```

In this case, a one-unit increase in LIVE_VISION is associated with an increase of 6.53 points in the WORK_LIFE_BALANCE_SCORE. The R-squared value indicates that approximately 21.89% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with LIVE_VISION.

```{r}
model_social_network <- lm(WORK_LIFE_BALANCE_SCORE ~ SOCIAL_NETWORK, data = train_data)
summary(model_social_network)

```

In this case, a one-unit increase in SOCIAL_NETWORK is associated with an increase of 6 points in the WORK_LIFE_BALANCE_SCORE. The R-squared value indicates that approximately 16.94% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with SOCIAL_NETWORK.

```{r}
model_supporting_others <- lm(WORK_LIFE_BALANCE_SCORE ~ SUPPORTING_OTHERS, data = train_data)
summary(model_supporting_others)

```

In this case, a one-unit increase in SUPPORTING_OTHERS is associated with an increase of 7.66 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value indicates that approximately 30.44% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with SUPPORTING_OTHERS.

```{r}
model_core_circle <- lm(WORK_LIFE_BALANCE_SCORE ~ CORE_CIRCLE, data = train_data)
summary(model_core_circle)

```

In this case, a one-unit increase in CORE_CIRCLE is associated with an increase of 8.1 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value indicates that approximately 25.82% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with CORE_CIRCLE.

```{r}
model_achievement <- lm(WORK_LIFE_BALANCE_SCORE ~ ACHIEVEMENT, data = train_data)
summary(model_achievement)

```

In this case, a one-unit increase in ACHIEVEMENT is associated with an increase of 9.16 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value indicates that approximately 31.49% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with ACHIEVEMENT.

```{r}
model_sufficient_income <- lm(WORK_LIFE_BALANCE_SCORE ~ SUFFICIENT_INCOME, data = train_data)
summary(model_sufficient_income)

```

A one-point increase in Sufficient income was associated with an increase of 41 points in WLB. The p-value shows significance and the R squared shows that this variable is responsible for 16% of the variance in the data.

```{r}
model_personal_awards <- lm(WORK_LIFE_BALANCE_SCORE ~ PERSONAL_AWARDS, data = train_data)
summary(model_personal_awards)

```

In this case, a one-unit increase in PERSONAL_AWARDS is associated with an increase of 7.4 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value indicates that approximately 25.77% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with PERSONAL_AWARDS.

```{r}
model_time_for_passion <- lm(WORK_LIFE_BALANCE_SCORE ~ TIME_FOR_PASSION, data = train_data)
summary(model_time_for_passion)

```

In this case, a one-unit increase in TIME_FOR_PASSION is associated with an increase of 8.5 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value indicates that approximately 26.66% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with TIME_FOR_PASSION.

```{r}
model_todo_completed <- lm(WORK_LIFE_BALANCE_SCORE ~ TODO_COMPLETED, data = train_data)
summary(model_todo_completed)

```

In this case, a one-unit increase in TODO_COMPLETED is associated with an increase of 9.3 points in the WORK_LIFE_BALANCE_SCORE. The R-squared value indicates that approximately 29.59% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with TODO_COMPLETED.

```{r}
model_donation <- lm(WORK_LIFE_BALANCE_SCORE ~ DONATION, data = train_data)
summary(model_donation)

```

A one-unit increase in DONATION is associated with an increase of 11.2 points in the WORK_LIFE_BALANCE_SCORE.

The R-squared value of 0.2111 indicates that approximately 21.11% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with DONATION.

```{r}
model_lost_vacation <- lm(WORK_LIFE_BALANCE_SCORE ~ LOST_VACATION, data = train_data)
summary(model_lost_vacation)

```

In this case, a one-unit increase in LOST_VACATION is associated with a decrease of 3.3 points in the WORK_LIFE_BALANCE_SCORE.

R-squared: The R-squared value of 0.07337 indicates that approximately 7.34% of the variability in the WORK_LIFE_BALANCE_SCORE can be explained by the linear relationship with LOST_VACATION. This is not highly significant.

#### Results

From the results linear regressions individually measured, the following areas impact the WLB with a higher statistical significance and influence:\
\
Based on Coefficients:

-   FRUITS_VEGGIES: 14.3643

-   BMI_RANGE: -23.4981

-   STRESS_LEVEL: -12.3096

-   LIVE_VISION: 9.1198

-   SUPPORTING_OTHERS: 8.0669

-   CORE_CIRCLE: 8.0646

-   ACHIEVEMENT: 9.2095

-   SUFFICIENT_INCOME: 41.0881

-   TIME_FOR_PASSION: 9.1939

-   TODO_COMPLETED: 9.3226

-   DONATION: 11.1891

Among these variables, SUFFICIENT_INCOME, BMI_RANGE, FRUITS_VEGGIES and STRESS_LEVEL have been found to significantly influence the scores of WLB. This outcome is slightly different than what could be seen from the heatmap.

### Collective linear regression

```{r}
model <- lm(WORK_LIFE_BALANCE_SCORE ~ AGE + GENDER + FRUITS_VEGGIES + BMI_RANGE + DAILY_STEPS +
            SLEEP_HOURS + DAILY_STRESS + FLOW + WEEKLY_MEDITATION + DAILY_SHOUTING +
            LIVE_VISION + SOCIAL_NETWORK + SUPPORTING_OTHERS + CORE_CIRCLE + ACHIEVEMENT +
            SUFFICIENT_INCOME + PERSONAL_AWARDS + TIME_FOR_PASSION + TODO_COMPLETED +
            DONATION + LOST_VACATION, data = train_data)
summary(model)

```

Based on the collective model's coefficients, BMI (-17 points) and Sufficient Income (18 points) were the most noticable drivers of change.

#### Results

When combined, previously high coefficients turned out to be lower.

In this case, the effects of multicollinearity are more harsh and the coefficients of correlated variables can become unstable and change when other variables are included in the model. This can result in a decrease in the magnitude of the coefficients compared to their individual regression.

### Random Forest regression

```{r}
# Specify the column index of WLB in your dataset
response_col <- 24  

# Perform random forest regression
rf_model <- randomForest(selected_data[, -response_col],  
# Input features
                         selected_data[, response_col],    
# Response variable
                         ntree = 100,                      
# Number of trees in the forest
                         mtry = sqrt(ncol(selected_data) - 1),  
# Number of variables randomly sampled as candidates at each split
                         importance = TRUE)                
# Calculate variable importance
# Make predictions on the test set
predictions <- predict(rf_model, test_data[, -response_col])
```

## Evaluation

```{r}
# Evaluate the model
mse <- mean((predictions - test_data[, response_col])^2)  # Mean Squared Error
rmse <- sqrt(mse)  # Root Mean Squared Error
rsquared <- cor(predictions, test_data[, response_col])^2  # R-squared

# Print evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("R-squared:", rsquared, "\n")


```

```{r}
# Print variable importance
var_importance <- importance(rf_model)
print(var_importance)

```
